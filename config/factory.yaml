# Usage: facet run -c config/factory.yaml:FACET:Install
#
# Configuration Help
#
# ${{key}} - interpolate value of 'key'
# ${{key1:key2}} - interpolate value of 'key2' ('key2' is a value of 'key1')
# $name or ${name} - interpolate value of environment variable 'name'

Install:
    install:
        filename: data/install/american-english
        nrows: 10000
        # Any additional key:value pairs are forwarded to pandas.read_csv()
        encoding: utf-8
        # skiprows: 0

Install2:
    install:
        filename: data/umls/MRCONSO.RRF
        delimiter: "|"
        cols: 14
        nrows: 38000
        # Any additional key:value pairs are forwarded to pandas.read_csv()
        encoding: utf-8
        # skiprows: 0

UMLSInstall:
    install:
        filename: data/umls
        nrows: 38000
        # Any additional key:value pairs are forwarded to pandas.read_csv()
        encoding: utf-8
        # skiprows: 0

###############################################################################

# FACET() object
FACET:
    class: facet
    # Matcher() object
    matcher: ${{SimstringMatcher}}
    # Tokenizer() object
    tokenizer: ${{Tokenizer}}
    # Formatter() object
    formatter: ${{Formatter}}
    # If set, install in-memory first, then copy to database
    use_proxy_install: false

# FACET() object
UMLSFACET:
    class: umlsfacet
    # Database() object
    conso_db: ${{MemoryDictDatabase}}
    # Database() object
    cuisty_db: ${{MemoryDictDatabase}}
    # Matcher() object
    matcher: ${{SimstringMatcher}}
    # Tokenizer() object
    tokenizer: ${{Tokenizer}}
    # Formatter() object
    formatter: ${{Formatter}}
    # If set, install in-memory first, then copy to database
    use_proxy_install: false

###############################################################################

# Matcher() object
SimstringMatcher:
    class: simstring
    # Database() object
    db: ${{MemoryDictDatabase}}
    # Database() object
    cache_db: ${{MemoryDictDatabase}}
    alpha: 0.7
    similarity: jaccard
    # Ngram() object
    ngram: ${{CharacterNgram}}

# Matcher() object
ElasticsearchSimstringMatcher:
    class: elasticsearch-simstring
    # 'db' represents parameters forwarded to ElasticsearchDatabase()
    db:
        index: test
        settings:
            number_of_shards: 1
            number_of_replicas: 0
            max_result_window: 10000
        hosts: localhost:9200
        access_mode: c
        use_pipeline: true
        connect: true
        # Use bulk API
        thread_count: 4
        # Use streaming API
        # stream: true
        # Any additional key:value pairs are forwarded to elasticsearch.Elasticsearch()
    # Database() object
    cache_db: ${{MemoryDictDatabase}}
    alpha: 0.7
    similarity: jaccard
    # Ngram() object
    ngram: ${{CharacterNgram}}

# Matcher() object
MongoSimstringMatcher:
    class: mongo-simstring
    # 'db' represents parameters forwarded to MongoDatabase()
    db:
        database: test
        collection: strings
        host: localhost
        port: 27017
        access_mode: c
        use_pipeline: true
        connect: true
        # Any additional key:value pairs are forwarded to pymongo.MongoClient()
    # Database() object
    cache_db: ${{MemoryDictDatabase}}
    alpha: 0.7
    similarity: jaccard
    # Ngram() object
    ngram: ${{CharacterNgram}}

# Matcher() object
RediSearchSimstringMatcher:
    class: redisearch-simstring
    # 'db' represents parameters forwarded to RediSearchDatabase()
    db:
        index: test
        host: localhost
        port: 6379
        access_mode: c
        use_pipeline: true
        chunk_size: 10000
        connect: true
        # Any additional key:value pairs are forwarded to redis.Redis()

# Matcher() object
ElasticsearchFuzzyMatcher:
    class: elasticsearch-fuzzy
    # 'db' represents parameters forwarded to ElasticsearchDatabase()
    db:
        index: test
        settings:
            number_of_shards: 1
            number_of_replicas: 0
            max_result_window: 10000
        hosts: localhost:9200
        access_mode: c
        use_pipeline: true
        connect: true
        # Use bulk API
        thread_count: 4
        # Use streaming API
        # stream: true
        # Any additional key:value pairs are forwarded to elasticsearch.Elasticsearch()
    # Database() object
    cache_db: ${{MemoryDictDatabase}}
    # The following parameters are specific to Elasticsearch fuzzy matching
    rank: true
    exact_match: false
    fuzziness: AUTO
    prefix_length: 0
    max_expansions: 50
    transpositions: true

# Matcher() object
RediSearchMatcher:
    class: redisearch-match
    # 'db' represents parameters forwarded to RediSearchDatabase()
    db:
        index: test
        host: localhost
        port: 6379
        access_mode: c
        use_pipeline: true
        chunk_size: 10000
        connect: true
        # Any additional key:value pairs are forwarded to redis.Redis()
    alpha: 0.7
    similarity: jaccard

###############################################################################

# Database() object
FileDictDatabase:
    class: dict
    filename: db/testing
    access_mode: c
    use_pipeline: true
    connect: true

# Database() object
MemoryDictDatabase:
    class: dict
    connect: true

# Database() object
RedisDatabase:
    class: redis
    # Database index
    n: 0
    host: localhost
    port: 6379
    access_mode: c
    use_pipeline: true
    connect: true
    serializer: ${{PickleSerializer}}
    # Any additional key:value pairs are forwarded to redis.Redis()

# Database() object
SQLiteDatabase:
    class: sqlite
    # uri':memory:' or filename
    uri: ':memory:'
    # uri: test
    table: test
    access_mode: c
    use_pipeline: true
    connect: true
    serializer: ${{PickleSerializer}}
    # Any additional key:value pairs are forwarded to sqlite3.connect()
    check_same_thread: false

# Database() object
ElasticsearchDatabase:
    class: elasticsearch
    index: test
    settings:
        number_of_shards: 1
        number_of_replicas: 0
        max_result_window: 10000
    hosts: localhost:9200
    access_mode: c
    use_pipeline: true
    connect: true
    # Use bulk API
    thread_count: 4
    # Use streaming API
    # stream: true
    # Any additional key:value pairs are forwarded to elasticsearch.Elasticsearch()

# Database() object
MongoDatabase:
    class: mongo
    database: test
    collection: strings
    host: localhost
    port: 27017
    access_mode: c
    use_pipeline: true
    connect: true
    # Any additional key:value pairs are forwarded to pymongo.MongoClient()

# Database() object
RediSearchDatabase:
    class: redisearch
    index: test
    host: localhost
    port: 6379
    access_mode: c
    use_pipeline: true
    chunk_size: 10000
    connect: true
    # Any additional key:value pairs are forwarded to redis.Redis()

###############################################################################

# Serializer() object
Serializer:
    # class: null
    class: json
    # class: yaml
    # class: string
    # class: arrow
    encoding: utf-8
    # Normalization form (NFC, NFKC, NFD, NFKD).
    form: NFKD

# Serializer() object
StringSJSerializer:
    class: stringsj
    delimiter: "|"
    encoding: utf-8
    # Normalization form (NFC, NFKC, NFD, NFKD).
    form: NFKD

# Serializer() object
PickleSerializer:
    class: pickle
    encoding: utf-8
    # Normalization form (NFC, NFKC, NFD, NFKD).
    form: NFKD
    protocol: 4

# Serializer() object
CloudpickleSerializer:
    class: cloudpickle
    encoding: utf-8
    # Normalization form (NFC, NFKC, NFD, NFKD).
    form: NFKD
    protocol: 4

###############################################################################

# Tokenizer() object
# Sentencizes in newlines
Tokenizer:
    # class: null
    # class: whitespace
    class: alphanumeric
    window: 1
    min_token_length: 2
    converters: lower
    use_stopwords: true
    stopwords: null

# Tokenizer() object
# Sentencizes in newlines
SymbolTokenizer:
    class: symbol
    window: 1
    min_token_length: 2
    converters: lower
    use_stopwords: true
    stopwords: null
    # Symbols for tokenization
    # symbols: "\t\n\r\f\v .,?!\"'-:;()[]{}",
    # Symbols to include in addition to 'symbols'
    include_symbols: null
    # Symbols to exclude from 'symbols'
    exclude_symbols: null

# Tokenizer() object
SpaCyTokenizer:
    class: spacy
    window: 1
    min_token_length: 2
    converters: lower
    use_stopwords: true
    stopwords: null
    # Phrase chunker, 'noun' uses nouns only, 'noun_chunks'
    # uses basic noun chunking, 'pos_chunks' uses parts-of-speech
    # for chunking, None uses window-based tokenization. If chunking
    # is enabled then 'window', 'stopwords', and 'min_token_length'
    # parameters are not used.
    chunker: null
    # chunker: nouns
    # chunker: noun_chunks
    # chunker: pos_chunks
    lemmatizer: True
    language: en

# Tokenizer() object
NLTKTokenizer:
    class: nltk
    window: 1
    min_token_length: 2
    converters: lower
    use_stopwords: true
    stopwords: null
    sentencizer: punctuation
    # sentencizer: line
    tokenizer: treebank
    # tokenizer: punctuation
    # tokenizer: space
    # tokenizer: whitespace
    # Phrase chunker, 'noun' uses nouns only, 'noun_chunks'
    # uses basic noun chunking, 'pos_chunks' uses parts-of-speech
    # for chunking, None uses window-based tokenization. If chunking
    # is enabled then 'window', 'stopwords', and 'min_token_length'
    # parameters are not used.
    chunker: null
    # chunker: nouns
    # chunker: noun_chunks
    # chunker: pos_chunks
    lemmatizer: snowball
    # lemmatizer: null
    # lemmatizer: ci
    # lemmatizer: isri
    # lemmatizer: lancaster
    # lemmatizer: porter
    # lemmatizer: rslps
    # lemmatizer: wordnet
    language: english

###############################################################################

# Ngram() object
CharacterNgram:
    class: character
    n: 3
    boundary_length: 0

###############################################################################

# Formatter() object
Formatter:
    # class: null
    # class: csv
    class: json
    # class: yaml
    # class: xml
    # class: pickle
    # class: cloudpickle
